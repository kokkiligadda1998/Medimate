{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646d7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79c1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =r\"/Users/saikiran/Documents/NLP/MedNer-main/annotated_dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807950ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_acronyms = {\n",
    "    'Activity': 'ACT',\n",
    "    'Administration': 'ADM',\n",
    "    'Age': 'AGE',\n",
    "    'Area': 'ARA',\n",
    "    'Biological_attribute': 'BAT',\n",
    "    'Biological_structure': 'BST',\n",
    "    'Clinical_event': 'CLE',\n",
    "    'Color': 'COL',\n",
    "    'Coreference': 'COR',\n",
    "    'Date': 'DAT',\n",
    "    'Detailed_description': 'DET',\n",
    "    'Diagnostic_procedure': 'DIA',\n",
    "    'Disease_disorder': 'DIS',\n",
    "    'Distance': 'DIS',\n",
    "    'Dosage': 'DOS',\n",
    "    'Duration': 'DUR',\n",
    "    'Family_history': 'FAM',\n",
    "    'Frequency': 'FRE',\n",
    "    'Height': 'HEI',\n",
    "    'History': 'HIS',\n",
    "    'Lab_value': 'LAB',\n",
    "    'Mass': 'MAS',\n",
    "    'Medication': 'MED',\n",
    "    'Nonbiological_location': 'NBL',\n",
    "    'Occupation': 'OCC',\n",
    "    'Other_entity': 'OTH',\n",
    "    'Other_event': 'OTE',\n",
    "    'Outcome': 'OUT',\n",
    "    'Personal_background': 'PER',\n",
    "    'Qualitative_concept': 'QUC',\n",
    "    'Quantitative_concept': 'QUC',\n",
    "    'Severity': 'SEV',\n",
    "    'Sex': 'SEX',\n",
    "    'Shape': 'SHA',\n",
    "    'Sign_symptom': 'SIG',\n",
    "    'Subject': 'SUB',\n",
    "    'Texture': 'TEX',\n",
    "    'Therapeutic_procedure': 'THP',\n",
    "    'Time': 'TIM',\n",
    "    'Volume': 'VOL',\n",
    "    'Weight': 'WEI'\n",
    "}\n",
    "acronyms_to_entities = {v: k for k, v in entity_to_acronyms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfee3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the JSON file for reading\n",
    "with open(os.path.join(data_dir, \"annotated_data.json\"),'r',encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79948cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_punctuation(token):\n",
    "    \"\"\"\n",
    "    Removes trailing punctuation from a token.\n",
    "    \"\"\"\n",
    "    while token and re.search(r'[^\\w\\s\\']', token[-1]):\n",
    "        token = token[:-1]\n",
    "  return token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f91548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "\n",
    "    regex_match = r'[^\\s\\u200a\\-\\u2010-\\u2015\\u2212\\uff0d]+'  # r'[^\\s\\u200a\\-\\—\\–]+'\n",
    "\n",
    "    tokens = []\n",
    "    start_end_ranges = []\n",
    "\n",
    "    sentence_breaks = []\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    for sentence in text.split('\\n'):\n",
    "        words = [match.group(0) for match in re.finditer(regex_match, sentence)]\n",
    "        processed_words = list(map(remove_trailing_punctuation, words))\n",
    "        sentence_indices = [(match.start(), match.start() + len(token)) for match, token in\n",
    "                            zip(re.finditer(regex_match, sentence), processed_words)]\n",
    "\n",
    "        # Update the indices to account for the current sentence's position in the entire text\n",
    "        sentence_indices = [(start_idx + start, start_idx + end) for start, end in sentence_indices]\n",
    "\n",
    "        start_end_ranges.extend(sentence_indices)\n",
    "        tokens.extend(processed_words)\n",
    "\n",
    "        sentence_breaks.append(len(tokens))\n",
    "\n",
    "        start_idx += len(sentence) + 1\n",
    "    return tokens, start_end_ranges, sentence_breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9702a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['CASE', 'A', '28', 'year', 'old', 'previously', 'healthy', 'man', 'presented', 'with', 'a', '6', 'week', 'history', 'of', 'palpitations', 'The', 'symp'], [(0, 4), (6, 7), (8, 10), (11, 15), (16, 19), (20, 30), (31, 38), (39, 42), (43, 52), (53, 57), (58, 59), (60, 61), (62, 66), (67, 74), (75, 77), (78, 90), (92, 95), (96, 100)], [16, 18])\n"
     ]
    }
   ],
   "source": [
    "for doc_id, doc in data.items():\n",
    "    print(split_text(doc['text'][:100]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f34ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_token(tokens, tags, token_pos, entity):\n",
    "\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    tag = entity_to_acronyms[entity]\n",
    "    \n",
    "    if token_pos > 0 and f'{tag}' in tags[token_pos - 1]:        \n",
    "            tags[token_pos] = f'I-{tag}'\n",
    "    elif tokens[token_pos] not in stop_words:\n",
    "            tags[token_pos] = f'B-{tag}'\n",
    "            \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92a416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bio_files(output_file_path, tokens, tags, sentence_breaks):\n",
    "\n",
    "    # Write the tags to a .bio file\n",
    "    with open(output_file_path,'w',encoding='utf-8') as f:\n",
    "        for i in range(len(tokens)):\n",
    "            token = tokens[i].strip()\n",
    "            if token:\n",
    "                if i in sentence_breaks:\n",
    "                    f.write(\"\\n\")\n",
    "                f.write(f\"{tokens[i]}\\t{tags[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55184bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ann_to_bio(data, output_dir, filtered_entities=[]):\n",
    "    \n",
    "  \n",
    "    \n",
    "    if os.path.exists(output_dir):\n",
    "        # Delete the contents of the directory\n",
    "        shutil.rmtree(output_dir)\n",
    "    # Recreate the directory\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    \n",
    "    for file_id in data:\n",
    "        text = data[file_id]['text']\n",
    "        annotations = data[file_id]['annotations']\n",
    "        \n",
    "        # Tokenizing\n",
    "        tokens, token2text, sentence_breaks = split_text(text)\n",
    "\n",
    "        # Initialize the tags\n",
    "        tags = ['O'] * len(tokens)\n",
    "\n",
    "        ann_pos = 0\n",
    "        token_pos = 0\n",
    "\n",
    "        while ann_pos < len(annotations) and token_pos < len(tokens):\n",
    "\n",
    "            label = annotations[ann_pos]['label']\n",
    "            start = annotations[ann_pos]['start']\n",
    "            end = annotations[ann_pos]['end']\n",
    "\n",
    "            if filtered_entities:\n",
    "                if label not in filtered_entities:\n",
    "                    # increment to access next annotation\n",
    "                    ann_pos += 1\n",
    "                    continue\n",
    "            \n",
    "            ann_word = text[start:end]\n",
    "\n",
    "            # find the next word that fall between the annotation start and end\n",
    "            while token_pos < len(tokens) and token2text[token_pos][0] < start:\n",
    "                \n",
    "                token_pos += 1\n",
    "\n",
    "            if tokens[token_pos] == ann_word or \\\n",
    "                ann_word in tokens[token_pos] or \\\n",
    "                re.sub(r'\\W+', '', ann_word) in re.sub(r'\\W+', '', tokens[token_pos]):\n",
    "                tag_token(tokens, tags, token_pos, label)\n",
    "            elif ann_word in tokens[token_pos - 1] or \\\n",
    "                ann_word in tokens[token_pos - 1] or \\\n",
    "                re.sub(r'\\W+', '', ann_word) in re.sub(r'\\W+', '', tokens[token_pos - 1]):\n",
    "                tag_token(tokens, tags, token_pos - 1, label)\n",
    "            else:\n",
    "                print(tokens[token_pos], tokens[token_pos - 1], ann_word, label)\n",
    "\n",
    "            # increment to access next annotation\n",
    "            ann_pos += 1\n",
    "\n",
    "        # write to bio file\n",
    "        write_bio_files(os.path.join(output_dir, f\"{file_id}.bio\"), tokens, tags, sentence_breaks)\n",
    "    print(\"Conversion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd7815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete\n"
     ]
    }
   ],
   "source": [
    "convert_ann_to_bio(data, data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
